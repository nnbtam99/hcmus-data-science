{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Họ và tên: **Nguyễn Ngọc Băng Tâm**\n",
    "\n",
    "MSSV: **1712747**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import urllib.robotparser # Kiểm tra file robot.txt có được phép crawl không"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Api_data/'):\n",
    "    os.makedirs('Api_data/')\n",
    "    \n",
    "if not os.path.exists('Crawl_data/'):\n",
    "    os.makedirs('Crawl_data/')\n",
    "    \n",
    "if not os.path.exists('Crawl_urls/'):\n",
    "    os.makedirs('Crawl_urls/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ý tưởng chọn tập mẫu:**\n",
    "\n",
    "- B1: Tìm và lưu lại **top 500 từ khóa được tìm kiếm nhiều nhất (popular searches) trong 24 giờ gần nhất** vào file `popular_keywords.txt`. \n",
    "- B2: Lọc lại 300 từ khóa và lấy các url danh sách các playlist thuộc từ khóa đó, lưu vào file `query_popular_playlists.txt`. \n",
    "- B3: Parse playlist url từ các url kết quả truy vấn và lưu vào file `popular_playlists.txt`\n",
    "- B4: Với mỗi playlist url, parse user url và lưu vào file `popular_users.txt`\n",
    "- B5: Với mỗi playlist url, parse 5 track url thuộc playlist đó và lưu vào file `popular_tracks.txt`\n",
    "\n",
    "**Lưu ý:**\n",
    "- Ta sẽ xử lý các dòng trùng lắp bằng cách thêm các url vào một tập hợp (set) và lưu file từ các phần tử của tập hợp đó\n",
    "- Lưu tất cả các url trên vào các file text trong thư mục trung gian `Crawl_urls`, khi cần thực hiện parse HTML chỉ cần duyệt lấy url và parse thành từng đối tượng user, track và playlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Tạo robot parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url('https://soundcloud.com/robots.txt')\n",
    "rp.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Lấy các url cần thiết trước khi parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B1. Lấy 500 từ khóa được tìm kiếm nhiều nhất trong 24 giờ qua\n",
    "Thời điểm đồ án chọn là 24 giờ qua tính đến ngày 05/11/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_keywords(popular_keywords_file, sleep_time=1):\n",
    "    \"\"\" \n",
    "    Get the latest 500 hot keywords\n",
    "    \"\"\"\n",
    "    session = HTMLSession()\n",
    "    \n",
    "    with open(popular_keywords_file, 'w') as out:\n",
    "        for page in range(1, 6):\n",
    "            # sleep before request a new session\n",
    "            time.sleep(sleep_time)\n",
    "            base_url = f'https://soundcloud.com/popular/searches/{page}'\n",
    "            r = session.get(base_url)\n",
    "            for url in r.html.absolute_links:\n",
    "                if \"soundcloud.com/search\" in url and rp.can_fetch('*', url):\n",
    "                    out.write(f'{url}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_popular_keywords('Crawl_urls/popular_keywords.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B2. Lấy 300 url chứa kết quả truy vấn các playlist phổ biến"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_query_results(popular_keywords_file, data_type):\n",
    "    \"\"\" \n",
    "    Get query results of playlists from the latest 300 keyword (based on the data_type argumewith open(f'{data_type}_url.txt', 'w') as out:nt)\n",
    "    - popular_keywords_file: txt file of the urls of 300 popular keywords\n",
    "    - data_type: type of objects to get url\n",
    "    \"\"\"\n",
    "    \n",
    "    # dict - equivalent Soundcloud terminology of each data_type\n",
    "    soundcloud_term = {'users': 'people',\n",
    "                   'tracks': 'sounds',\n",
    "                   'playlists': 'sets'}\n",
    "    \n",
    "    with open(f'Crawl_urls/query_popular_{data_type}.txt', 'w') as out:\n",
    "        with open(popular_keywords_file, 'r') as inp:\n",
    "            cnt = 0\n",
    "            for line in inp:\n",
    "\n",
    "                if cnt == 300:\n",
    "                    break\n",
    "        \n",
    "                if rp.can_fetch('*', line):\n",
    "                    preprocessed_line = line.replace(\"?q\", f'/{soundcloud_term[data_type]}?q')\n",
    "                    out.write(f'{preprocessed_line}')\n",
    "                    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_popular_query_results('Crawl_urls/popular_keywords.txt', 'playlists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B3. Lấy url của từng playlist dựa trên url kết quả truy vấn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_type_urls(data_type, data_regex, sleep_time=1):\n",
    "    \"\"\"\n",
    "    Get urls of each user, playlist, track\n",
    "    - data_type: user / playlist / track\n",
    "    - data_regex: regular expression to match the data_type url\n",
    "    \"\"\"\n",
    "    session = HTMLSession()\n",
    "\n",
    "    with open(f'Crawl_urls/popular_{data_type}.txt', 'w') as out:\n",
    "        with open(f'Crawl_urls/query_popular_{data_type}.txt', 'r') as inp:\n",
    "            for line in inp:\n",
    "                if rp.can_fetch('*', line):\n",
    "                     # sleep before request a new session\n",
    "                    time.sleep(sleep_time)\n",
    "                    r = session.get(line)\n",
    "                     \n",
    "                    for url in r.html.absolute_links:                \n",
    "                        if 'search' not in url and re.search(data_regex, url) is not None:\n",
    "                            out.write(f'{url}\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_regex = 'https?:\\/\\/soundcloud.com\\/\\S+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playlist\n",
    "get_data_type_urls('playlists', playlist_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xóa các dòng playlist trùng lắp và kiểm tra số lượng playlist duy nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PLAYLIST\n",
    "unique_playlists = set()\n",
    "\n",
    "# Thực hiện xóa các dòng trùng lắp\n",
    "def remove_duplicates():\n",
    "    with open('Crawl_urls/unique_popular_playlists.txt', 'w') as out:\n",
    "        with open('Crawl_urls/popular_playlists.txt', 'r') as inp:\n",
    "            for line in inp:\n",
    "                unique_playlists.add(line)\n",
    "        for playlist in unique_playlists:\n",
    "            out.write(f'{playlist}')\n",
    "            \n",
    "remove_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique playlists: 2722\n"
     ]
    }
   ],
   "source": [
    "with open('Crawl_urls/unique_popular_playlists.txt', 'r') as inp:\n",
    "    cnt = 0\n",
    "    for line in inp:\n",
    "        cnt += 1\n",
    "        \n",
    "print(f'unique playlists: {cnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B4: Với mỗi playlist url, parse user url và lưu vào file `popular_users.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_from_playlist_urls():\n",
    "    \n",
    "    # delete duplicate lines in user_file\n",
    "    unique_users = set()\n",
    "    with open('Crawl_urls/unique_popular_users.txt', 'w') as out:\n",
    "        with open('Crawl_urls/unique_popular_playlists.txt', 'r') as inp:\n",
    "            for line in inp:\n",
    "                if rp.can_fetch('*', line):\n",
    "                    line_lst = line.split('/')[:4]\n",
    "                    user_url = '/'.join(line_lst)\n",
    "                    unique_users.add(user_url)\n",
    "                    \n",
    "        for user in unique_users:\n",
    "            out.write(f'{user}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_users_from_playlist_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B5: Với mỗi playlist url, parse 5 track url thuộc playlist đó và lưu vào file `popular_tracks.txt`\n",
    "\n",
    "Do các url của Soundcloud có nhúng Javascript, ta thực hiện download mã nguồn trang web bằng selenium và parse các đối tượng cần lấy bằng regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracks_from_playlist_urls(driver, sleep_time=1):\n",
    "\n",
    "    unique_tracks = set()\n",
    "    \n",
    "    with open('Crawl_urls/unique_popular_tracks.txt', 'w') as out:\n",
    "        with open('Crawl_urls/unique_popular_playlists.txt', 'r') as inp:\n",
    "            for line in inp:\n",
    "                if rp.can_fetch('*', line):\n",
    "                    # sleep before requesting a new session\n",
    "                    time.sleep(sleep_time)\n",
    "                    driver.get(line)\n",
    "                    page_src = browser.page_source\n",
    "\n",
    "                    # find playlist\n",
    "                    playlist_regex = '\\[{\"artwork_url\".+\"}\\]'\n",
    "                    pos = re.search(playlist_regex, page_src)\n",
    "                \n",
    "                    if pos is not None:\n",
    "                        raw_str = page_src[pos.start():pos.end()]\n",
    "                \n",
    "                        # find track url\n",
    "                        track_search = r'\"permalink_url\":\"https:\\S+?\"{1}'\n",
    "                        tracks = re.findall(track_search, raw_str)\n",
    "\n",
    "                        if len(unique_tracks) == 6000:\n",
    "                            break\n",
    "                    \n",
    "                        for track in tracks:\n",
    "                            if track.count('/') == 4:\n",
    "                                track = track[len('\"permalink_url\":\"'):-1]\n",
    "                                unique_tracks.add(track)\n",
    "                    else:\n",
    "                        print(f'{line} has problems')\n",
    "                else:\n",
    "                    print(f\"Can't fetch {line}\")\n",
    "                \n",
    "        \n",
    "        for track in unique_tracks:\n",
    "            out.write(f'{track}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://soundcloud.com/18-user-user/sets/die-very-rough\n",
      " has problems\n",
      "https://soundcloud.com/18-user-user/sets/big-bootie-mix-bottie\n",
      " has problems\n",
      "https://soundcloud.com/2020-songs/sets/what-do-you-know-about-love-pop-smoke\n",
      " has problems\n",
      "https://soundcloud.com/dll-repost-2/sets/what-do-you-know-about-love-pop-smoke-2\n",
      " has problems\n",
      "https://soundcloud.com/2020-songs/sets/whoopty-cj\n",
      " has problems\n",
      "https://soundcloud.com/18-user-user/sets/big-bootie-mix\n",
      " has problems\n",
      "https://soundcloud.com/2020-songs/sets/easy-life-sangria-ft-arlo-parks\n",
      " has problems\n",
      "https://soundcloud.com/18-user-user/sets/drip-like-me\n",
      " has problems\n",
      "https://soundcloud.com/philipjcastillo/sets/dolly-lil-uzi-vert\n",
      " has problems\n",
      "https://soundcloud.com/2020-songs/sets/tik-tok\n",
      " has problems\n",
      "https://soundcloud.com/319963465-79694589-weeee/sets/trippie-redd\n",
      " has problems\n",
      "https://soundcloud.com/richardmloleary/sets/capitalize-off-pain\n",
      " has problems\n",
      "https://soundcloud.com/user-1920291817281910/sets/laugh-now-cry-later\n",
      " has problems\n",
      "https://soundcloud.com/2020-songs/sets/savage-love\n",
      " has problems\n",
      "https://soundcloud.com/dll-repost-2/sets/lets-link\n",
      " has problems\n",
      "https://soundcloud.com/906040655-all-in/sets/gunsmoke-youngboy\n",
      " has problems\n",
      "https://soundcloud.com/dll-repost-2/sets/big-bootie-mix-18\n",
      " has problems\n",
      "https://soundcloud.com/r_graham09/sets/lil-mosey\n",
      " has problems\n",
      "https://soundcloud.com/156667773287464638-i/sets/slowed-and-reverb\n",
      " has problems\n",
      "https://soundcloud.com/2020-songs/sets/franchise-remix-travis-scott\n",
      " has problems\n"
     ]
    }
   ],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "browser = webdriver.Chrome('./chromedriver', options=chrome_options)\n",
    "\n",
    "get_tracks_from_playlist_urls(browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tham khảo \n",
    "- https://www.oreilly.com/library/view/web-scraping-with/9781491910283/ch01.html\n",
    "- https://towardsdatascience.com/a-practical-guide-to-exploratory-data-analysis-spotify-dataset-d8f703da663e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
